# MedRAX æ¶æ§‹ç¾ä»£åŒ–æ–¹æ¡ˆ

## ğŸ“Š 1. åŠŸèƒ½æ“´å±•ï¼šCXR + EKG å¤šæ¨¡æ…‹é†«å­¸æ¨ç†

### ç•¶å‰ç‹€æ³
- âœ… CXRï¼ˆèƒ¸éƒ¨ X å…‰ï¼‰å®Œæ•´æ”¯æ´
- âœ… 13 å€‹å°ˆæ¥­é†«å­¸å·¥å…·
- âŒ ç¼ºå°‘å¿ƒé›»åœ–ï¼ˆEKG/ECGï¼‰æ”¯æ´
- âŒ å¤šæ¨¡æ…‹æ•´åˆä¸å®Œå–„

### æ“´å±•æ–¹æ¡ˆ

#### 1.1 EKG åˆ†æå·¥å…·æ£§
```
EKG Tools Layer:
â”œâ”€â”€ Signal Processing
â”‚   â”œâ”€â”€ ECG-SQI (ä¿¡è™Ÿå“è³ªè©•ä¼°)
â”‚   â”œâ”€â”€ R-peak Detection
â”‚   â””â”€â”€ HRV Analysis (å¿ƒç‡è®Šç•°æ€§)
â”œâ”€â”€ ML Models
â”‚   â”œâ”€â”€ ResNet-ECG (12-lead ECG classification)
â”‚   â”œâ”€â”€ Transformer-ECG (ç•°å¸¸æª¢æ¸¬)
â”‚   â””â”€â”€ ArrhythmiaNet (å¿ƒå¾‹ä¸æ•´åˆ†é¡)
â””â”€â”€ Clinical Reasoning
    â”œâ”€â”€ ECG-BERT (è‡¨åºŠæ–‡æœ¬å ±å‘Š)
    â”œâ”€â”€ PatternMatcher (å·²çŸ¥å¿ƒå¾‹åœ–è­œå°æ¯”)
    â””â”€â”€ RiskAssessment (é¢¨éšªè©•åˆ†)
```

#### 1.2 CXR + EKG è¯åˆæ¨ç†
```python
# ä¾‹ï¼šè‚ºç‚æ‚£è€…åŒæ™‚æœ‰å¿ƒå¾‹ä¸æ•´
CXR Finding: "Pneumonia in right lower lobe"
EKG Finding: "Atrial fibrillation with RVR"

Joint Reasoning:
- Sepsis risk assessment (è€ƒæ…®å¿ƒå¾‹ä¸æ•´)
- Treatment interactions (è‚ºç‚è—¥ç‰© Ã— å¿ƒè‡Ÿè—¥ç‰©)
- Monitoring priorities (å„ªå…ˆç›£æ§å¿ƒç‡ vs æ°§é£½å’Œåº¦)
```

---

## ğŸ—ï¸ 2. Agent æ¶æ§‹ç¾ä»£åŒ–

### ç•¶å‰æ¶æ§‹åˆ†æ
```
ç¾ç‹€ï¼šç°¡æ˜“ç·šæ€§æµç¨‹
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”
â”‚LLM (æ€è€ƒ)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Tool Selection   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Tool Execution   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Return Result    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç¾ä»£åŒ–æ–¹æ¡ˆï¼šLangGraph + State Management

#### 2.1 å¤šå±¤æ¶æ§‹
```
v0.3.0: LangGraph é‡æ§‹

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Medical Reasoning Agent v0.3          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚    Query Understanding & Routing     â”‚    â”‚
â”‚  â”‚  (Determine: CXR? EKG? Both?)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚               â”‚                              â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚       â”‚                â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€vâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚CXR Path â”‚      â”‚EKG Path  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â”‚                â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Multi-Modal Fusion Layer   â”‚            â”‚
â”‚  â”‚ (è¯åˆæ¨ç†&ç›¸äº’é©—è­‰)         â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚       â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Clinical Decision Support Layer   â”‚      â”‚
â”‚  â”‚ (é¢¨éšªè©•ä¼°ã€å»ºè­°ã€è§£é‡‹)            â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚       â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  Response Format â”‚                      â”‚
â”‚  â”‚  (çµæ§‹åŒ–è¼¸å‡º)     â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.2 LangGraph ç‹€æ…‹æ©Ÿåˆ¶
```python
# Agent State è¨­è¨ˆ
class MedicalReasoningState(TypedDict):
    """
    Medical multi-modal reasoning state
    """
    # è¼¸å…¥
    query: str
    uploaded_images: List[Image]  # CXRã€EKG æ³¢å½¢
    
    # è·¯ç”±æ±ºç­–
    query_type: Literal["cxr", "ekg", "combined"]
    modalities_detected: List[str]
    
    # CXR è·¯å¾‘
    cxr_findings: Dict[str, Any]  # åˆ†å‰²ã€åˆ†é¡ã€å®šä½çµæœ
    cxr_report: str
    
    # EKG è·¯å¾‘
    ekg_signal: np.ndarray  # åŸå§‹ä¿¡è™Ÿ
    ekg_features: Dict[str, float]  # HR, QT, ST segments ç­‰
    ekg_classification: Dict[str, float]  # å„é¡å¿ƒå¾‹åˆ†æ•¸
    ekg_report: str
    
    # è¯åˆå±¤
    combined_context: str  # CXR + EKG è‡¨åºŠèƒŒæ™¯
    
    # æ±ºç­–æ”¯æŒ
    clinical_decision: str
    risk_assessment: Dict[str, float]
    recommendations: List[str]
    
    # è§£é‡‹æ€§
    reasoning_trace: List[str]  # æ¨ç†éç¨‹è¨˜éŒ„
    
    # è¼¸å‡º
    final_response: str
    confidence_scores: Dict[str, float]
```

#### 2.3 Tool Use Protocol (ç¾ä»£åšæ³•)
```python
# æ–°å¼ Tool Calling (Anthropic/OpenAI æ¨™æº–)
class ToolUseNode:
    async def process(self, state: MedicalReasoningState):
        # LLM æ±ºå®šç”¨ä»€éº¼å·¥å…·
        response = await llm.apredict(
            system=system_prompt,
            messages=state.messages,
            tools=[
                # å·¥å…·ä»¥ JSON Schema å®šç¾©
                {
                    "name": "cxr_analysis",
                    "description": "åˆ†æèƒ¸éƒ¨ X å…‰",
                    "input_schema": {
                        "type": "object",
                        "properties": {
                            "image_id": {"type": "string"},
                            "analysis_types": {
                                "type": "array",
                                "items": {"enum": ["segmentation", "classification", "qa"]}
                            },
                            "user_marked_region": {
                                "type": "object",
                                "description": "ä½¿ç”¨è€…åœˆå‡ºçš„å€åŸŸ"
                            }
                        }
                    }
                },
                {
                    "name": "ekg_analysis",
                    "description": "åˆ†æå¿ƒé›»åœ–",
                    "input_schema": {...}
                }
            ]
        )
        
        # è™•ç†å·¥å…·èª¿ç”¨
        if response.tool_use:
            tool_result = await self.execute_tool(response.tool_use)
            # æ›´æ–°ç‹€æ…‹ï¼Œç¹¼çºŒè¿´åœˆ
        else:
            # LLM å·²æä¾›æœ€çµ‚ç­”æ¡ˆ
            return response.text
```

---

## ğŸ¨ 3. UI/UX ç¾ä»£åŒ–æ–¹æ¡ˆ

### 3.1 å¯¦æ™‚æ¨™è¨»èˆ‡ç©ºé–“æ„ŸçŸ¥äº’å‹•

#### ç•¶å‰é™åˆ¶
```
ç¾ç‹€ (Gradio):
1ï¸âƒ£ User: ä¸Šå‚³ X å…‰
2ï¸âƒ£ Agent: è‡ªå‹•åˆ†æå…¨åœ–
3ï¸âƒ£ User: é–±è®€å ±å‘Š
âŒ ç„¡æ³•æŒ‡å®šæ„Ÿèˆˆè¶£çš„å€åŸŸ
âŒ ç„¡æ³•å¯¦æ™‚æ¨™è¨»
âŒ äº’å‹•æœ‰å»¶é²
```

#### æ”¹é€²æ–¹æ¡ˆ A: å¢å¼·ç‰ˆ Gradio UI
```python
# Gradio + Canvas ç¹ªåœ–å±¤
import gradio as gr
from gradio_canvas import Canvas

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    with gr.Row():
        # å·¦: å½±åƒ + ç¹ªåœ–å±¤
        with gr.Column():
            image_display = gr.Image(
                label="Medical Image",
                type="filepath"
            )
            # ä½¿ç”¨è€…å¯åœ¨ä¸Šé¢ç•«åœˆ/æ–¹æ¡†
            annotation_canvas = Canvas(
                label="Mark regions of interest",
                value=None
            )
            
        # å³: å¯¦æ™‚çµæœ
        with gr.Column():
            findings_output = gr.Textbox(
                label="AI Findings",
                interactive=False,
                lines=20
            )
            confidence_chart = gr.BarChart(
                label="Confidence Scores",
                x="finding",
                y="confidence"
            )
    
    # æäº¤æŒ‰éˆ•
    analyze_btn = gr.Button("Analyze Marked Regions", variant="primary")
    
    # äº‹ä»¶è™•ç†
    def analyze_with_context(image, annotations):
        """
        Args:
            image: PIL Image æˆ–è·¯å¾‘
            annotations: {
                "regions": [
                    {"type": "rect", "coords": [x1, y1, x2, y2]},
                    {"type": "circle", "coords": [cx, cy, r]},
                ],
                "user_question": "What's this?"
            }
        """
        # ç™¼é€åˆ° Agent ä¸¦åŠ å…¥ç©ºé–“ä¸Šä¸‹æ–‡
        result = agent.analyze_with_focus(
            image=image,
            focused_regions=annotations["regions"],
            user_query=annotations.get("user_question", "")
        )
        return result.findings, result.confidence_scores
    
    analyze_btn.click(
        fn=analyze_with_context,
        inputs=[image_display, annotation_canvas],
        outputs=[findings_output, confidence_chart]
    )
```

### 3.2 å¤šæ¨¡æ…‹è¼¸å…¥èˆ‡å¯¦æ™‚äº’å‹•

```python
# æ”¯æŒå¤šç¨®è¼¸å…¥æ–¹å¼
class MultiModalInterface:
    """
    æ”¯æ´ï¼š
    1. å½±åƒæ¨™è¨» (Annotation)
    2. èªéŸ³è¼¸å…¥ (Voice)
    3. æ–‡æœ¬æŸ¥è©¢ (Text)
    4. æ™‚é–“åºåˆ—äº’å‹• (Temporal)
    """
    
    async def handle_user_interaction(self, 
                                     images: List[np.ndarray],
                                     annotations: Dict[str, Any],
                                     voice_input: Optional[bytes],
                                     text_query: str):
        """
        çµ±ä¸€è™•ç†å¤šæ¨¡æ…‹è¼¸å…¥ï¼Œæ§‹é€ è±å¯Œçš„ä¸Šä¸‹æ–‡
        """
        
        context = {
            "visual": {
                "cxr": images[0] if images else None,
                "marked_regions": annotations,
                "user_attention": self._extract_saliency(annotations)
            },
            "linguistic": {
                "text_query": text_query,
                "voice_query": self._transcribe(voice_input),
                "intent": self._classify_intent(text_query)
            },
            "temporal": {
                "timestamp": datetime.now(),
                "session_id": self.session_id,
                "interaction_history": self.history[-5:]  # æœ€è¿‘5æ¬¡äº’å‹•
            }
        }
        
        # ç™¼é€åˆ° Agent
        response = await self.agent.reason_multimodal(context)
        return response
```

### 3.3 Model å¦‚ä½•ç²çŸ¥ç”¨æˆ¶æ¨™è¨»çš„å€åŸŸ

#### æ–¹æ¡ˆ 1: å€åŸŸåµŒå…¥ (Region Embedding)
```python
class RegionAwareAgent:
    """
    è®“ LLM ç†è§£ç”¨æˆ¶åœˆå‡ºçš„å€åŸŸ
    """
    
    async def process_marked_regions(self, 
                                    image: np.ndarray,
                                    marked_regions: List[Dict]):
        """
        æ¨™è¨» â†’ ç‰¹å¾µæå– â†’ LLM æç¤ºè©
        """
        
        # Step 1: ç‚ºæ¯å€‹æ¨™è¨˜å€åŸŸæå–è¦–è¦ºç‰¹å¾µ
        region_features = []
        for region in marked_regions:
            roi = self._extract_roi(image, region['coords'])
            
            # ä½¿ç”¨ Vision Encoder æå–ç‰¹å¾µ
            features = self.vision_encoder.encode(roi)
            
            region_features.append({
                "id": region['id'],
                "location": self._describe_location(region['coords']),
                "visual_features": features,
                "user_question": region.get('question', '')
            })
        
        # Step 2: æ§‹é€ å¢å¼·æç¤ºè©
        system_prompt = f"""
        ç”¨æˆ¶åœ¨åŒ»å­¦å½±åƒä¸Šæ¨™è¨˜äº† {len(region_features)} å€‹æ„Ÿèˆˆè¶£å€åŸŸã€‚
        
        æ¨™è¨˜å€åŸŸä¿¡æ¯ï¼š
        {self._format_regions(region_features)}
        
        è«‹é‡å°é€™äº›æ¨™è¨˜å€åŸŸé€²è¡Œæ·±å…¥åˆ†æã€‚
        """
        
        # Step 3: ç™¼é€çµ¦ LLM
        response = await self.llm.apredict(
            messages=[
                SystemMessage(content=system_prompt),
                HumanMessage(content="Please analyze the marked regions"),
            ]
        )
        
        return response

    def _format_regions(self, regions: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–å€åŸŸä¿¡æ¯çµ¦ LLM
        
        ä¾‹ï¼š
        å€åŸŸ 1: å³ä¸‹è‚ºé‡ (294,382) - (412,521)
        ç”¨æˆ¶å•é¡Œ: "This looks abnormal?"
        è¦–è¦ºç‰¹å¾µ: [0.23, 0.45, ..., 0.89]
        """
        formatted = []
        for i, region in enumerate(regions):
            desc = f"""
            å€åŸŸ {i+1}: {region['location']}
            åº§æ¨™: {region['coords']}
            ç”¨æˆ¶ç–‘å•: {region.get('user_question', 'ç„¡')}
            """
            formatted.append(desc)
        return "\n".join(formatted)
```

#### æ–¹æ¡ˆ 2: è¦–è¦ºåŸºç¤æ¨¡å‹ (Visual Foundation Model)
```python
class VisualGroundingAgent:
    """
    ä½¿ç”¨è¦–è¦ºåŸºç¤æ¨¡å‹ç†è§£ç©ºé–“æŒ‡ä»£
    ä¾‹: LLaVA-Grounding, Claude Vision, GPT-4V
    """
    
    async def ground_user_markup(self,
                                image: np.ndarray,
                                marked_regions: List[Dict],
                                user_query: str):
        """
        ç›´æ¥è®“ LLM çœ‹åœ– + æ¨™è¨˜
        """
        
        # Step 1: åœ¨åœ–åƒä¸Šç¹ªè£½ç”¨æˆ¶æ¨™è¨˜ï¼ˆè¦–è¦ºåŒ–ï¼‰
        annotated_image = self._draw_annotations(image, marked_regions)
        
        # Step 2: ç™¼é€çµ¦å¤šæ¨¡æ…‹ LLM (GPT-4V, Claude 3.5)
        # è®“æ¨¡å‹ç›´æ¥çœ‹åˆ°æ¨™è¨˜å’ŒåŸåœ–
        message = await self.vision_llm.apredict(
            images=[annotated_image],  # æ¨™è¨˜å·²ç•«ä¸Šå»
            text=f"""
            ç”¨æˆ¶å·²åœ¨é†«å­¸å½±åƒä¸Šæ¨™è¨˜äº† {len(marked_regions)} å€‹å€åŸŸï¼ˆç´…è‰²æ¡†ï¼‰ã€‚
            
            ç”¨æˆ¶æŸ¥è©¢: {user_query}
            
            è«‹åŸºæ–¼æ¨™è¨˜çš„å€åŸŸé€²è¡Œè©³ç´°åˆ†æã€‚
            """
        )
        
        return message
```

---

## ğŸ¤– 4. Agent æ¨¡å¼ï¼šStandalone vs MCP vs Copilot Integration

### ç•¶å‰ï¼šStandalone Agent (v0.2)
```
User â†’ Gradio UI â†’ Agent â†’ Tools â†’ Response
```

### é¸é … A: Model Context Protocol (MCP) Server
```
å°‡ MedRAX ä½œç‚º MCP Server
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Claude / Any LLM Client      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ MCP Protocol
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MedRAX MCP Server            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ cxr_analysis                  â”‚
â”‚ â€¢ ekg_analysis                  â”‚
â”‚ â€¢ clinical_decision_support     â”‚
â”‚ â€¢ image_grounding               â”‚
â”‚ â€¢ report_generation             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å„ªé»**ï¼š
- âœ… ç”± Claude/GPT æ§åˆ¶æ¨ç†æµç¨‹
- âœ… æ›´éˆæ´»çš„å¤šæ­¥æ¨ç†
- âœ… ç„¡é™ä¸Šä¸‹æ–‡æ­·å²
- âœ… æ›´å¥½çš„æŒ‡ä»¤è¿½è¹¤

**ç¼ºé»**ï¼š
- âŒ éœ€è¦å…¼å®¹å®¢æˆ¶ç«¯
- âŒ ç¶²çµ¡å»¶é²

### é¸é … B: Copilot Integration (A2A æ¨¡å¼)
```
VS Code Copilot â†â†’ MedRAX Agent (via SDK)

åœ¨ VS Code ä¸­ç›´æ¥ä½¿ç”¨ MedRAX:
User: @medrax analyze this CXR
                â†“
         Copilot Router
                â†“
         MedRAX Agent
                â†“
         Return results
         (inline in chat)
```

**éœ€è¦å¯¦ç¾**ï¼š
```typescript
// VS Code Extension (Copilot Participant)
vscode.chat.createChatParticipant('medrax', {
    invoke: async (request: vscode.ChatRequest) => {
        // èª¿ç”¨ MedRAX Agent
        const result = await medraxAgent.analyze({
            images: request.attachedImages,
            query: request.prompt
        });
        return result;
    }
});
```

**å„ªé»**ï¼š
- âœ… ç„¡ç¸«é›†æˆé–‹ç™¼ç’°å¢ƒ
- âœ… è¨ªå• Copilot å®Œæ•´åŠŸèƒ½
- âœ… æ”¯æŒä»£ç¢¼å’Œæ–‡æª”åŒæ™‚åˆ†æ
- âœ… æœ¬åœ°åŸ·è¡Œï¼ˆä½å»¶é²ï¼‰

**ç¼ºé»**ï¼š
- âŒ é™åˆ¶åœ¨ VS Code ç’°å¢ƒ
- âŒ éœ€è¦ VS Code æ“´å±•é–‹ç™¼

### é¸é … C: Hybrid (æ¨è–¦) ğŸ†
```
v0.4.0: Unified Multi-Interface Agent

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          MedRAX Core Agent               â”‚
â”‚  (LangGraph + Multi-Modal Reasoning)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚              â”‚
â”Œâ”€â”€â”€vâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€vâ”€â”€â”€â”€â”
â”‚ Gradio â”‚   â”‚MCP Serverâ”‚   â”‚Copilot  â”‚
â”‚   UI   â”‚   â”‚          â”‚   â”‚Plugin   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å„ªé»ï¼š
- åŒæ™‚æ”¯æŒæ‰€æœ‰ä½¿ç”¨å ´æ™¯
- ç”¨æˆ¶å¯é¸æ“‡æœ€é©åˆçš„ä»‹é¢
- å…±ç”¨åŒä¸€å€‹å¼·å¤§çš„ Agent æ ¸å¿ƒ
```

---

## ğŸ“‹ 5. å¯¦ç¾è·¯ç·šåœ–

### Phase 1: åŠŸèƒ½æ“´å±• (v0.2.0) - 2025Q1
- [ ] EKG ä¿¡è™Ÿè™•ç†å·¥å…·
- [ ] ECG åˆ†é¡æ¨¡å‹é›†æˆ
- [ ] CXR + EKG è¯åˆæ¨ç†æç¤ºè©

### Phase 2: æ¶æ§‹ç¾ä»£åŒ– (v0.3.0) - 2025Q1-Q2
- [ ] LangGraph é‡æ§‹
- [ ] Tool Use Protocol å¯¦ç¾
- [ ] ç‹€æ…‹ç®¡ç†å„ªåŒ–
- [ ] å¤šæ¨¡æ…‹è·¯ç”±é‚è¼¯

### Phase 3: UI äº’å‹•å¢å¼· (v0.3.5) - 2025Q2
- [ ] æ¨™è¨» Canvas æ•´åˆ
- [ ] å€åŸŸæ„ŸçŸ¥æ¨ç†
- [ ] å¯¦æ™‚åé¥‹æ”¹é€²

### Phase 4: Agent æ¨¡å¼ (v0.4.0) - 2025Q2-Q3
- [ ] MCP Server å¯¦ç¾
- [ ] VS Code Copilot æ“´å±•
- [ ] Hybrid ç•Œé¢ç®¡ç†

---

## ğŸ› ï¸ 6. æŠ€è¡“æ·±åº¦

### 6.1 Region Awareness å¯¦ç¾ç´°ç¯€

```python
class SpatialAwareAnalyzer:
    """
    ç†è§£ç”¨æˆ¶æ¨™è¨˜çš„å€åŸŸä½ç½®å’Œé‡è¦æ€§
    """
    
    def analyze_with_spatial_context(self,
                                    image: np.ndarray,
                                    marked_regions: List[Bbox],
                                    anatomical_context: Dict):
        """
        ä¾‹ï¼šç”¨æˆ¶åœˆå‡ºå³ä¸‹è‚ºé‡
        
        ç³»çµ±å›æ‡‰æ‡‰è©²ï¼š
        1. èªªæ˜æ‰¾åˆ°çš„ç•°å¸¸ä½ç½®
        2. èˆ‡å·²çŸ¥è§£å‰–ç‰¹å¾µæ¯”è¼ƒ
        3. æä¾›å®šä½ä¿¡æ¯ (using anatomical landmarks)
        4. è©•ä¼°è‡¨åºŠæ„ç¾©
        """
        
        # 1. è§£å‰–å®šä½
        for region in marked_regions:
            anatomical_location = self.anatomical_localizer(
                region.bbox,
                anatomical_context
            )
            print(f"ç”¨æˆ¶æ¨™è¨˜ä½ç½®: {anatomical_location}")
            # e.g., "Right lower lobe, lateral segment"
        
        # 2. æª¢ç´¢ç›¸ä¼¼ç—…ä¾‹
        similar_cases = self.case_retriever.find_similar(
            marked_regions=marked_regions,
            query_image=image
        )
        
        # 3. ç”Ÿæˆå®šä½åŒ–å ±å‘Š
        report = self.report_generator.generate(
            findings=findings,
            spatial_context=marked_regions,
            similar_cases=similar_cases
        )
        
        return report
```

### 6.2 EKG ä¿¡è™Ÿè™•ç†æµç¨‹

```python
class ECGAnalyzer:
    """
    12-lead ECG å®Œæ•´åˆ†æ
    """
    
    async def full_analysis(self, ecg_signal: np.ndarray):
        """
        Input: (12, 5000) - 12 leads, 5000 samples
        """
        
        # 1. ä¿¡è™Ÿå“è³ªè©•ä¼°
        quality = self.assess_signal_quality(ecg_signal)
        
        # 2. ç‰¹å¾µæå–
        features = {
            "heart_rate": self.calculate_heart_rate(ecg_signal),
            "pr_interval": self.measure_pr(ecg_signal),
            "qrs_duration": self.measure_qrs(ecg_signal),
            "qt_interval": self.measure_qt(ecg_signal),
            "st_segment": self.analyze_st(ecg_signal),
            "t_wave": self.analyze_t_wave(ecg_signal),
        }
        
        # 3. æ·±åº¦å­¸ç¿’åˆ†é¡
        classifications = await self.ecg_classifier(ecg_signal)
        # Output: {
        #   "normal": 0.92,
        #   "atrial_fibrillation": 0.05,
        #   "left_ventricular_hypertrophy": 0.02,
        #   ...
        # }
        
        # 4. è‡¨åºŠæ¨ç†
        clinical_report = self.reasoning_engine.generate_report(
            features=features,
            classifications=classifications,
            quality=quality
        )
        
        return {
            "features": features,
            "classifications": classifications,
            "clinical_report": clinical_report,
            "quality_score": quality
        }
```

---

## ğŸ“Š ç¸½çµèˆ‡å»ºè­°

| ç¶­åº¦ | ç¾ç‹€ | ç›®æ¨™ (v0.4) | å„ªå…ˆç´š |
|------|------|----------|--------|
| **åŠŸèƒ½** | CXR only | CXR + EKG | ğŸ”´ é«˜ |
| **Agent** | ç°¡æ˜“ç·šæ€§ | LangGraph + MCP | ğŸ”´ é«˜ |
| **UI** | éœæ…‹ä¸Šå‚³ | å‹•æ…‹æ¨™è¨» + å¤šæ¨¡æ…‹ | ğŸŸ¡ ä¸­ |
| **é›†æˆ** | Gradio ç¨ç«‹ | Hybrid (Gradio+MCP+Copilot) | ğŸŸ¡ ä¸­ |

**å»ºè­°é †åº**ï¼š
1. âœ… v0.2.0: EKG å·¥å…· + è¯åˆæ¨ç† (2 weeks)
2. âœ… v0.3.0: LangGraph é‡æ§‹ (3 weeks)
3. âœ… v0.3.5: æ¨™è¨» Canvas (1 week)
4. âœ… v0.4.0: MCP + Copilot (4 weeks)

